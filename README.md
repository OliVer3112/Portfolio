# Portfolio

## Datacamp
This minor was the first time I had to work with python. As a complete beginner the Datacamp courses were really useful, as they went from the most basic teachings, like creating variables, using libraries and loops; to more concrete elements of the data science field like how to create a K-Nearest Neighbor model, how to clean data or how to validate a model. The main advantages of this courses for me were, apart from learning how to code in python, it's relation with the minor. Many times what we were doing at the course was the same as what the lectures were  about, so as a way of understanding everything better were very useful.

My progress can be seen in [profile progress overview](https://user-images.githubusercontent.com/94642037/148603346-27e81a79-3962-4e6e-8e96-fc812803bcd7.png)
and in [datacamp course progress](https://github.com/OliVer3112/Portfolio/blob/3c5a61acdb3083141379980b6151bda34eccfbae/Datacamp/Assignments%20-%20DataCamp%20Learn.pdf)

## Reflection and Evaluation
#### Situation
In this minor the project I was assigned to was project Wheels, the task assigned to the group was to improve the outcomes of collecting data of wheelchair basketball players from the Dutch national team through sensors in the wheelchair.
#### Task
This was my first time working in a task with such level of importance and with a group of more than 3 people. Knowing this I couldn’t take a main role as I knew almost anything about what to do or how to do it, so I had to listen many times. At first, I couldn’t add much, though throughout the project this was improved. Even though my situation, I expected myself to be helpful and at least have some general knowledge on all the aspects in which the project is involved. 
#### Action
To make my role in the project more relevant I tried to work on many different aspects of it such as model preparation, research in models and data preprocessing. First of all I had to focus more on my Datacamp courses, as I needed to get some python background before I could try making any coding. With the machine learning methods happens the same as with python, first I needed to conduct some research. I finally participated in the creation of presentations, the first machine learning models and preparation of rotation labels.
#### Results
Throughout the development of the project with the help of my research, some code from lectures and datacamp I was able to complete my tasks. At first I was working together with Daan in a script of MatLab which allowed us to take the MatLab files where the dataset was to a csv. Another of my contributions was the initation to machine learning models, as I created the K-Nearest Neighbors model which was one of the first models we made for the project. When we wanted to see how would linear models work on our dataset I researched all the information about it and also wrote a code for a SGD Classifier which was the method chosen. The team wanted to try work on linear models, so I decided to take the task, researched on the best possible model and implemented it.  Later I focused on the detections of rotations, which would help set a decent amount of them for further classification. 
#### Reflection
I think that my contribution in some parts due to lack of knowledge and not a complete domain of the language could have been better, but I think I tried my hardest for getting results good enough and tried to make something relevant for the project. I think in the future I should try to get more involved in the decision making and understand better the line in which the project is going, with this I mean to know hat has to be done and why, this could help me to take a more important role in future projects.

	
## Reflection on own Learning Objectives
#### Situation
For the first time thanks to this minor, I got hands into python, also was my first contact with data science and my first project with this level of seriousness, so the situation was really challenging from the first moment
#### Task
From the first moment my objectives were to get some basic knowledge of python and learn everything I could of machine learning methods, as for me the field of Artificial Intelligence is really interesting. I wanted to understand how a real project works too, I wanted to see myself working with people I don't know and from other fields of expertise so I could learnf rom their persectives
#### Action
In order to achieve my goals, I tried from the first moment to take notes of everything that was new for me, which made me have a slower pace compared to others but would help me to understand everything better. Also, I tried to assist to all the lectures and workshops possible as even if they were optional, I knew they would be helpful too. During the project as I wanted to understand many of the aspects of the minor, so I got involved in multiple different tasks like data preparation and machine learning models
#### Results
From the whole minor I get my first expirience in a real project, the seriousness it involves and also the importance of making quality work. Also I learnt how to ask for help when you think waht you are doing is going in the wrong direction. Another thing I've learnt is the basis of a new coding language like is python and the importance of data nowadays. Speaking about machine learning I've learnt how to implement, train, validate and evaluate models and also how to prepare data. 
#### Reflection
In my opinion my first approach to all these new things was good, but I think I went too slow on the python learning, which delayed me in other tasks. So definitely the approach I wanted to take was that, but I should have distinguished better in what parts I should be more conscientious in order to optimize time and efforts. I think that I worked well with my team, I think we ended up being comfortable with each other, giving feedback and receiving it appropiately and also asking for help when you need it. About data science I think its a really interesting field, though I would need to get deeper to know if it's something I want to keep on doing
	
## Evaluation on Group Project as a Whole
#### Situation
For this project our group had 2 electrical engineers, 1 software engineer, 1 informatic engineer and another groupmate studying human technology. This mix of future engineers has been part of team wheels during the development of this minor. Our project consisted in the classsification of wheelchair basketball movements
#### Task
The first task we had as a group was understanding each other strong points in order to make a good division of tasks. Once the structure of the group was set up we had to get into our problem, which is the classification of wheelchair basketball movements with the use of applied data science techniques. 
#### Action
First of all we introduced ourselves, our studying background and what we are more confortable working on. After, we tried to classify the problem we were facing, so that we could focus our later research on the right direction. Also to keep a work ethic we decided to follow a scrum methodology in order to track the progress of the project and its members. Once we had planned how we wanted to developed the project, we showed our problem owner what we wanted to achieve to see if it matched with what he was expecting from us. After this we decided to structure the projects' route and set up the tasks that needed to be done and assigned them to the members of the group. To track everybodies tasks and see the problems they were strugling with or the progress made, we decided to set up a daily meeting in the morning. In this meetings we wouldexplain what we did the previous day and what we expect to do for the next day. Later we got hands in the tasks: researching models, coding them and selecting the best ones. Finally we documented all of our advances in the research paper. 
#### Results
The results of the group were at first a plan of approach, which would set the path for the project. Moreover we made a code capable of working with a partially defined dataset. The dataset is partially defined, because theres's no definition of sprinting within wheelchair basketball. The code we made, uses two models to classify sprints of wheelchair basketball. Finally we have our research paper which contains all the discoveries made throughout the minor
#### Reflection
In my opinion the group was good, everybody tried to do its hardest and to communicate with each other. Of course, there were points of non-agreement, but I think that they were solved without any big problems. The work ethic generally was really good too, everybody knew what they had to do and tried its best to fullfill their tasks. Personally I think its a pitty we couldn't finish the rotation and collision detection, though I think that with what with the progress we made it wouldn't be something realy difficult.


## Research Project

#### Task definition
In professional sport, statistics are currently gaining a great deal of importance. Associations such as the NBA or FIFA use video tracking methods to keep statistics of recorded matches. This brings benefits to team managers as they are able to know the strengths and weaknesses of their players as well as their opponents, it also contributes to the prevention and detection of injuries and last but not least it is very useful for scouting. However the problem comes that not only is this video tracking system used as it is only used for match statistics, if you want to track the player more deeply you usually use very light sensors attached to the players equipment, these sensors work with players who are able to walk, however wheelchair athletes do not have that luck. This is where our project becomes relevant, as we propose the possibility of using IMU sensors, 2 in particular, one placed on a wheel and one placed under the player; and applying machine learning techniques to improve the statistical approach of the teams.

To start our project, the first thing we had to do was to get to know our problem owner and see what we had to start with, so that we could do some research on the subject and start working on it. After knowing the situation of the project we elaborated a [plan of approach](https://github.com/OliVer3112/Portfolio/blob/a996c03629a26de1a423ad64a0771a9a0c1beb4e/ProjectDocuments/Plan%20of%20approach.pdf) in which we decided to pose a main question and a series of sub-questions that we were interested in being able to answer at the end of the development of the project.

The main question we wanted to answer in our Plan of Approach is:
-  How can IMU data be used to identify wheelchair basketball-specific movements?

And the sub-questions raised by the team where:
-  Which form of data processing will be used? 
- Which specific movements can be detected? 
- Which sensor data is used for each movement? 
- Can movements be used to predict fatigue? 
- Can movements be used to detect overload? 

#### Evaluation
Throughout the project we've been trying many models, from the most basic ones like a Decision Tree to Neural Networks, until we found the models whose results were satisfactory. However, it took a long time to get to that point, due to various problems with the dataset. So we had to readjust the research questions to be more accurate with the final result, which we would be able to offer. 

This would be our final research question: 
- How can a RFC and a RNN be used to classify sprints in partially-defined IMU data? 

And this our sub-questions:
- Which form of data processing can be used to prepare the dataset?  
- Which sensor data can be used for the detection of the sprints?  

These new questions are more in line with the situation of our project, as the original ones were perhaps too ambitious, due to the inexperience we all had in the field of data science. In our [research paper](https://github.com/OliVer3112/Portfolio/blob/32b1b9c9ad884153f090b370e0ef4e5a4c79cc7f/ProjectDocuments/Research_Paper_Project_Wheels_V1%20(1).pdf) this questions have been already solved, as right now our models are capable of detecting sprints. Detection of other movements like rotatiosn, was in our scope and we only needed to adapt the existing code to achieve it. The results of this project can lead to really interesting future investigations, for example the detection and fatigue and overload this objective was on our scope at first. The main reason why we couldn't achieve this objectives was because we couldn´t finish the detection of all the required movements to study the fatigue and overload. Also we it could be interesting to study patterns in diferent paralysis levels.

#### Conclusions
In this project we had different problems with our dataset, which would be its diferential characteristic. One of the problems concerning our dataset was the lack of true positives, to fight this we decided to use two models to detect actions, and label them as a sprint when they "agreed". The models used to classsify sprints are Random Forest Classifier and a Recurrent Neural Network. When testing our model and checking the detected sprints we concluded a precision of 91.67% but an unkown recall too, this is due to the fact that our datasets didn´t have sprints tagged. In conclusion and answering our research question, from the obtained results it can be concluded that using an RFC and RNN model is possible to detect sprints in a partially defined IMU dataseet

#### Planning
Throughout the project time, team wheels has tried to be efficient and plan the necessary tasks for the correct development of the project, to do so we decided to use [Azure DevOps](https://dev.azure.com/Wheeeeeeeeeeeeeeels/wheels/_boards/board/t/wheels%20Team/Stories) as our scrum platform, where everybody would have their assigned task to be completed for each "sprint". To keep in contact with the group members and also update on the situation of their respective task, we decided to have a daily meeting at 9:30 in which we had to mention: what had been done the previous day and what is expected to be done for that day. Also once a week we decided to have a more in depth meeting, in order to be more specific and don't loose the track of the other tasks of the project. When the sprints finished (every 2 weeks) we would meet together to have a retrospective of the previous sprint, in these retrospectives we would state what we needed to improve, start and stop doing. In these meetings we would also create the user stories for the nect sprint and decide who is doing what.



## Predictive Analytics 

### Selecting a model
##### KNN
As a first approach of the project, we decided to split the group into machine learning and data team. The machine learning team was in charge of making the first approaches to any usable method we could use. So, we decided to try make work a decision tree, knn and gaussian naïve bayes. My task was to make a working model of K-nearest Neighbors. This selection was made since they were the first models we were told about and it would be a very naïve approach to the project in order to get used to the project, python and how machine learning works. Some research was conducted before to see some examples of the model, also the datacamp resources were helpful as they provided me with a [tutoral](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn) and some of the exercises which belonged to the to the [Supervised Learning with scikit-learn](https://app.datacamp.com/learn/courses/supervised-learning-with-scikit-learn))


##### SGD
At one moment of the project we wanted to try new models, so I decided to try and see how linear models adapted to our dataset. At first, i had to research on the  basic models usable on classification problems like [logistic regression](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python) and [support vector machines](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python) (SVM). Regarding the research of svm, I was lucky that there was a team who made learning lab presentation based on it, so I contacted them to ask for their researching sources, which would make my task a lot easier. Out of all the methods svm was really suitable as it’s a good method for two-class problems and also can perform well in time series dataframe, but for us there was only one problem which was the dataframe’s size, as it isn’t a good method for big amounts of data. Then I looked for similar options and I found out that the SGD was one model that could fit better out of the linear models we knew, as it's really similar to svm but can work on large datasets.

##### 2d CNN
After having our neural network and being in the middle of the 1d CNN development we decided to start working on the 2d CNN, as we thought it would be interesting to see if they could work on our dataset. I decided to try this task, as it I thought it would be a challenging task, though a  lot of time was required as I couldn’t find that much information about how to make our data work into a 2d CNN. The main problem with this method was how to adapt our data to a 2d CNN, at first one option I considered was converting the data into a 3d matrix, this method wouldn’t provide with any success, so I decided to make the graphs into images and then save them all, that way I would have images with which I could work on my model. Though there was a problem that made me decide dropping this model was the fact that our data wasn't labeled but also I lacked of a specific criteria to classify movements. Also we were told by the teachers in an internal presentation, that taking plots to images isn´t really recommendable, so we decided to left aside the 2d cnn and focus on our other modelsels

(Intentar enseñar la preparacion de los graficos en codigo)

### Configuring a model
##### KNN
The configuring of this model wasn't that difficult, it took a bit of time, beasue i needed tu catch up with the datacamp courses, but once i managed it wasn't difficult to make the model. I also used some tutorials on the internet to try find different approaches to the problem. For this model there aren't reall many hyperparameters to change so I made a loop that iterated over some possible values, in order to find the most accurate option.

##### SGD
At first the configuring of this model was a bit difficult as in the research mainly the information I found, was about Gradient Descent (like SGD but works on all data so its slower but more precise), though I finally found out the information about SGD Classifier. Once I had that information making the code was pretty simple, as I only had to split the data and define the classifier.

### Training
For the training of the model I decided to split the dataset into a training part of 80% of its size and the other 20% for the testing. SGD which is the only model in which I did tunning as with the KNN I didn´t know how to tune models, and at the moment I knew how to we already dropped the idea of using it. First I tried to manually change some hyperparameters to see the most important, until I decided to perform cross validation.

### Evaluation
(Explain its only for sgd)After the tunning the precision we got wasn't very good, so I decided to add the balance the data, to see if that would change the situation, though the change was minimum. As the main results were really bad we decided that there was no need of trying to balance the dataset by other methods which would be more time consuming as the reward wouldn't be worth.

## Domain knowledge


#### Introduction of the subject field
Today we live in the information age, where information comes and goes with just a few clicks. However, this information is of utmost importance in the workplace, ["The use of analytics is no longer limited to big companies with deep pockets. It’s now widespread, with 59% of enterprises using analytics in some capacity"](https://www.forbes.com/sites/forbesagencycouncil/2019/10/01/the-age-of-analytics-and-the-importance-of-data-quality/?sh=4e6e876c5c3c). The Applied Data Science minor has given me a first approach to the ins and outs of information manipulation and how to make predictions of events or classify information using large volumes of data and detecting patterns in it, in short the value of data science.
	"The use of analytics is no longer limited to big companies with deep pockets. It’s now widespread, with 59% of enterprises using analytics in some capacity" 

#### Literature research
As this was my first approach to data science and python, a minimum of research was required in order to understand the situation posed by the wheels project. The project consists in the detection of patterns and movement classification from IMU sensors' collected data. But before I started learning random things, I stated we needed an understanding of the problem I was facing, in order to find the points on which the project should focus. After some difficulties in finding information about the topic I found a paper that mentions the importance of activity trackers for wheelchair users [https://www.rehab.research.va.gov/jour/2016/536/pdf/jrrd-2016-01-0006.pdf], which, when supported by another paper found by my colleague Martijn [https://dl.acm.org/doi/abs/10.1145/2700648.2809845], which explain the difficulties of impaired athletes to track their activities and collect useful data of them, gave me a better understanding of the situation we were in.


After knowing the situation it was decided that we should all look for information about previous studies on the subject, among them I found [https://www.researchgate.net/publication/353663819_Machine_Learning_to_Improve_Orientation_Estimation_in_Sports_Situations_Challenging_for_Inertial_Sensor_Use#pf4] (uses Gaussian Naive Bayes algorithm, a logistic regression, a decisiontree algorithm, and a random forest algorithm) which is a study of IMU sensors in wheelchairs to detect inclinations in the trunk of athletes, I also found another paper [https://www.sciencedirect.com/science/article/pii/S1877050921011121] that made use of machine learning to detect fractal gait patterns in soldiers. On the other hand, other colleagues found different papers quite useful, among them are these, which make use of different methods from those I had found as convolutional neural networks [https://www.sciencedirect.com/science/article/pii/S2666307421000140] and recurrent [http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf].


After investigating the situation of the use of machine learning for motion detection and the use of sensors or data collection devices for wheelchair athletes, there were some interesting ideas in the use of different methods, but I still lacked the knowledge to develop the initiatives, among them and the one I focused on is the development of an image recognition model. First I had to do research about the simplest neural networks, for this I used the following link [link here](https://www.section.io/engineering-education/introduction-to-neural-networks/) because it helped me a lot to get started on the subject, then I could understand the convolutional neural networks, thanks to [this link](https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610) I could understand the difference between the different types of CNN 1d, 2d and 3d. Now I could focus on the 2d cnn, for this the following [tutorial](https://www.youtube.com/watch?v=pDdP0TFzsoQ&t=500s) and this [other tutorial](https://www.youtube.com/watch?v=EHuACSjijbI) were quite useful although some others I found differed in their implementation approach, but still helped me to find a common structure in the code


### Explain concepts
Dataset: To make use of machine learning it is essential to have a dataset, this consists of a series of contents, of any type (numbers, letters, etc.), within a table. Within the table, each column represents a particular variable called features, and each row represents a particular member of the dataset we are dealing with.
- Feature:Features are the basic building blocks of datasets, for example age, sex are some possible features you can find in a dataset. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning
- IMU: They are an electronic device that measures and reports the speed, orientation and gravitational forces of a device using a combination of accelerometers and gyroscopes. The name IMU comes from Inertial Measurement Unit. This sensors can measure in three axis (X, Y, Z) and they are the source of our raw data
  - Accelerometers: Device that can measure linear acceleration, can be also used for specific purposes such as inclination and vibration measurement.
  - Gyroscopes: Device that can measure and maintain the orientation and angular velocity of an object.

Data PreProcessing & preparation: The core of a machine learning project is its dataset, but before you can get hands into models you need to prepare and clean your dataset because data may be incomplete (attributes, values or both missing), noisy (data has errors and outliers) or inconsistent (data contains differences in codes or names)
- Data Cleaning: Process consisting in identifying data errors and correct them to create a complete and accurate dataset. In the case of our project we had some missing values due to game stops, players being substitued and also there were sitations where sensors stopped working. To solve this, it was needed to fill those blank spaces with NaN (Not a number) values which is an often used technique for data cleaning.
- Data Structuring: Identifying those input variables that are most relevant to the task, this means the selecon of features that will be more relevant, in our project they were wheel rotational speed and frame rotational speed for example. Sometimes its also needed to decompose features in simpler ones to help in capturing more specific relationships.
- Data Transforms & enrichment: Data often must be transformed to make it consistent and turn it into usable information. For example one technique worth mentioning is data balancing, this one has been essential for our project, as its necessary for classification problems and our aim was to classify movements from data. One of our problems was that our dataset was imbalanced, so there were a lot of points which wereN't sprints and few classified sprints. Because of this we couldnt classify appropiately with our models and we had to balnce the dataset so that the amount of sprints classified were similar to the amount of non-sprints
- Data Validation: Machine learning models are vulnerable to poor data quality, to avoid this its important to check the accuracy and quality of source data before training a new model version. Taht way you ensure that anomalies that are infrequent or manifested in incremental data are not silently ignored

Machine learning algorithms: These are the algorithms I've learnt to use during this minor. They are useful to perform a task without giving them explicit instruction, instead they learn the paterns they find in the data. There is a wide variety of algorithms that can be used, but they can be divided into:
- Regression: This type of algorithms will focus on predicting a specific value out of a continuous value, this means an exact number from a determined interval like 1-100 meters. This kind of algorithm wasn´t useful for my project
- Classification: This type of machine learning method is useful to classify data into a certain class.  For my project was the most useful, as we wanted to detect if something was a sprint, another example of the application of this method would be the captchas' where you need to choose the images containing bycicles, in that situation if you use a classification model it would recognise the images containing them. Some of algorithms we used for the project where: 
  - Logistic Regression
  - Naive Bayes
  - KNN (K-Nearest Kneighbors)
- Ranking: Used to predict an order, this type of algorithms are a central part of many information retrieval problems
- Clustering: Consists of a grouping of "similar" datapoints, this is really useful for discovering natural grouping and tendencies in data, but there are no classes to be predicted
- Neural Network: The name of this type of machine learning models come from the way they work. They are a  series of algorithms that mimic the operations of an animal brain, a “neuron” would be a mathematical function that collects and classifies information according to a specific architecture. The neural networks adapt to changing input, so the network generates the best possible result without needing to redesign the output criteria. The following are the types of neural networks researched for this project.
  - 1D CNN: A convolutional neural network in which the kernel runs in one dimension only. This type of neural network applies for time-series data, which means that the one dimension kernel will work with two dimensions data.
  - 2D CNN: Also known as image recognition, works as indicated by it's name, with image datasets. 2d CNN runs on a two dimensions kernel, which will stand three dimensions input data, this comes from the fact that images when turned into data will be decomposed pixel by pixel in a vector like this (Red, Green, Blue), coming from the RGB scale, although not all the times images will be on the RGB scale as they can be chaged into grayscale too.
 
These are some of the main elements which are essential for the understanding of a neural network:
- Tensors: Multi-dimensional arrays with a uniform type. They enable you to preserve and leverage that structure further, for individual layers or whole networks.
- Epochs: This is the number of times the forwardpropagation and backpropagation algorithms will be run.
- Learning rate: Defines how much your model will learn during each "iteration" (epoch) of training.
- Kernel: Filter applied to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map.
- Layers: Also known as Multi-Layer Perceptron of a Neural network, can be divided into 3 main layers:
  - Input: Initial data for the nn
  - Hidden: Where all the computation and mathematical algorithms and calculations take place				

Training: Consists of learning (determining) good values for all the weights by adjusting the parameters or weights of the model, and the bias from labeled examples adjust, in order to improve the models performance
- Train set: Really large subset of the data which is going to be used to train the model and build it
- Test set: Subset of the data that is used to test a machine learning program after it has been trained on an initial training data set
- Hyperparameters: These are variables whose value has been set before the training of a model, unlike parameters. They are key for the success, as they have to be tuned to fine the optimal values

Model evaluation: Once the model has been tested this process will help find the optimum values for the model to work at its best
- Cross validation: The goal of cross-validation is to test the model's ability to predict new data that was not used with independence of the partition between training and test data, as it will shuffle the data and choose one portion of the data as test set and the rest as training set. It's really helpful to flag problems like overfitting or underfitting
- K-Fold cross validation: The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into, it will choose one and take it as test set and the rest as training set, keep the score and repeat the process with another fold as a test set. Apart from that its the same as cross validation. It's usually used when your training set is small and you want to maximize it's size
  - Overfitting: Also known as high variance, occurs when the model is too specific and reacts to any small variation in the training data, it can occur when we are using too much parameters
  - Underfitting: Also known as high bias consists in a model being too simple to describe the data, opposite to overfitting can occur when there are too few parameters
			
- Loss: Is a penalty for a bad prediction, a number indicating how bad the model's prediction was on a single example. In case the loss is 0 then it means that the model's prediction was perfect. The goal of training a model is to find a set of weights and biases that have low loss, on average
  - Mean Squared Error: One of the most emblematic evaluation metric for regression models. It's main characteristics are the reduction of variance to data points and  its sensitivity to distant points (outliers). The lower it's values are the better your model. It's unit order is higher than the error unit as the error is squared, thats why many times Root Mean Squared Error is used instead.
  - Confusion matrix: One of the most common evaluation metrics for classification models. This method consist of a matrix of 2x2 dimensions. Inside the matrix  True positives (TP), positive class correctly detected; the False Positives (FP) positive class incorrectly detected; the False negatives (FN) negative class correctly detected and the True negatives (TN) can be found. This matrix will show the amount of points which were detected as what they "truly" are
    - Precision: Fraction of positive points among the truly classified points. The formula to calculate precision is: TP/(TP + FP)
    - Recall: Proportion of the positives detected correctly. The formula to calculate recall is: TP/(TP + FN)
    - Accuracy: Measures the proportion of true guesses by the model, it can make a model make better than it truly is because maybe detects a lot of true negatives but doesn't have true positives, which are the ones we want to increase. The formula to calculate accuracy is:(TP + TN)/(TP + TN + FP + FN)

## Data preparation
#### Sprint detection
At the very first steps of the project we first wanted to know how to detect sprints. In order to achieve that I worked with Jake on a code which would let us label sprints, this needed to be done before we could get into the machine learning model. What we decided as a first step would be a good idea was to set a limit for frame rotational speed as when it incresed we would state the end of the sprint and the start of a rotation. Also we would set peaks as sprints. This [code](link predict sprints) is the latest version of the sprint detection I made, this one differs from the one I made originally as I implement two simple models, one kNN and logistic regression. Later all this code has been improved by other members of the group with the help of the game videos we were missing at that moment to appropiately tag the actions of the data recordings.
#### Rotations
Throughout the project we've been trying to detect not only sprints but rotations and collisions. One of the tasks I wanted to take was the detection of collisions. This was really challenging as all the time I had been working on the research of machine learning methods and the implementation of them. At this moment sprints had already been detected by setting limit values of speed and some other variables in order to state what a sprint is, then when the values went over that limit during a minimum amount of time we would set does as a sprint. With rotations it would be similar, but I had no limit values yet as it was one of the first times anybody of the group tried to work on them. Our data was going through a low pass filter, and the time of the game was divided into chunkz (1 second = 100 chunkz), this caused the features to not be usable, because of the division in chunkz; to solve this we had to choose the max values out of all the chunkz. So at first I had to investigate and work with the code, as I didn´t really understand it well. Once I understood what everything was doing I would adjust the parameters which would suit better to the graphs peaks and conditions limiting the values of rotations, to do this I had to try al the possible combinations with elemmental operations and also try with some differencials. 

![](https://github.com/OliVer3112/Portfolio/blob/57ff0ace4c5628024b9195cf1dfcf2a17af2ab7a/Notebooks/RotationGraph.PNG)

Once I had the results that were more convincing for me I found out a problem where sometimes the rotations didn`t end in the same graph, as we were tracking the number of rotations that had started and stopped the sizes wouldn't match and I had to make some adjustments in the conditions for rotation finishing to solve this. This would be the first [script](codigo aqui). Later on we decided to add the rotation detection to another notebook. This new [notebook]() would detect both sprints and rotations but in a wider amount of data. The person in charge of sprints would add its part and I would add mine addapted to the new code. 



## Communication
### Presentations
In the course of time, we decided in my team who would be in charge of the respective presentation at least 4 days in advance. This would include asking colleagues about their progress, collecting that information and capturing it in a presentation, and then presenting that information on the day of the presentation.
##### Internal
This presentations were a way of updating the teachers and classmates about our progress. But the most important part of this presentatiotion was the pposibility of raising our doubts with colleagues who, if they have encountered the problem before or know the solution, may be able to help us. It is also very useful to be asked questions because we can find weaknesses in our approach and development of tasks. The following presentation on the [20th of September](https://github.com/OliVer3112/Portfolio/blob/b8edbe22a26cfb3fb4bd6d068659a65af5013815/Presentations/Internal20_Sep.pdf) was made entirely by myself, I also made the presentation on the [20th of december](https://github.com/OliVer3112/Portfolio/blob/f73a0a86f24c91a6bd259ef3a73c9c5479a4c65d/Presentations/Internal20_Dec.pdf), but this one I made it with Jake. My contributions were on the recap and present of the project, but both Jake and I gave a bit of feedback to each other in order to improve distribution and selection of presented information.
##### External
This presentations were useful for the problem owners to have an update of the state in which the project was. For this presentation on the [10th of Decemeber](https://github.com/OliVer3112/Portfolio/blob/4a6e0c7003ed5f5588fc4e52fea3fe94a31f05fa/Presentations/External10_Dec.pdf) I worked along with Martijn, my contributions were on the present situation of the project, results and also the next steps we were planning to take.

### Writing Paper
For this part of the project I mainly worked on the abstract, introduction and problem description, also I took part in the finishing of the disscussion and recommendation. Initially I was working alone in the abstract and helping the other mates in their respective parts. Later I was in charge of changing the introduction by myself and try to make it a fluent story with the problem description. After this, I was working with Jake in making the last revision to the introduction, problem description and research question and subquestions. Then we had a meeting after which, I ended up working with Daan in order to adapt the introduction and problem description to the new concept we had in mind. Finally I was in charge of making introduction a fluent story again and also had to work with Collin on the finishing of the Discussion and Recommendation. Also like the rest of the team I had to check the paper before every meeting and give some feedback to the parts I found that could be improved.
